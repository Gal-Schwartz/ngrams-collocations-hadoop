# Statistical Report  
## Collocation Extraction using MapReduce  
### English and Hebrew – Google N-Grams

---

## 1. Experimental Setup

The system was executed on Amazon EMR using Hadoop MapReduce.  
The experiments were conducted **separately for English and Hebrew**, and each language was processed independently through the three MapReduce jobs.

- **Job 1**: Counting unigrams and bigrams per (language, decade) with stop-word filtering  
- **Job 2**: Joining unigram and bigram counts and computing Log-Likelihood Ratio (LLR)  
- **Job 3**: Extracting the Top-100 collocations per (language, decade)

Stop-words were filtered using predefined lists for English and Hebrew.  
Local aggregation was implemented using **Hadoop Combiners**, as required.

---

## 2. Job 1 – Effect of Local Aggregation (Combiner)

Job 1 is the most communication-intensive stage, as it processes raw Google N-Grams data and emits large volumes of intermediate key-value pairs.

### English – Job 1

| Metric | Value |
|------|------|
| Map input records | **2,188,767,392** |
| Map output records (before combiner) | **903,390,118** |
| Combine input records | 903,390,118 |
| Combine output records | **224,870,507** |
| Reduction ratio (records) | **~75%** |
| Map output bytes | 27.7 GB |
| Shuffle bytes | **~3.0 GB** |

**Observation:**  
The combiner reduced the number of intermediate records by approximately **75%**.

The metric **Map output bytes** represents the total volume of intermediate key-value pairs produced by the mappers **after the map phase and serialization**, but **before local aggregation and shuffle**.
In contrast, **Shuffle bytes** represent the actual amount of data transferred over the network from mappers to reducers **after local aggregation (combiner), spill, and merge**.

This means that although **27.7 GB** of intermediate data were generated by the mappers, only **~3 GB** were actually transferred over the network.

This corresponds to a **network traffic reduction of approximately 90%**, demonstrating the critical role of the combiner in reducing communication cost.

---

### Hebrew – Job 1

| Metric | Value |
|------|------|
| Map input records | **296,470,071** |
| Map output records (before combiner) | **161,387,442** |
| Combine input records | 161,387,442 |
| Combine output records | **34,914,734** |
| Reduction ratio (records) | **~78%** |
| Map output bytes | 5.7 GB |
| Shuffle bytes | **~457 MB** |

**Observation:**  
Despite a smaller corpus, Hebrew shows an even stronger reduction due to higher repetition of terms.  
Local aggregation reduced intermediate data by **nearly 80%**.

The same behavior observed in the English corpus is also evident when processing Hebrew data.
For Job 1 (Hebrew), the logs report:

In this case, although approximately **5.7 GB** of intermediate data were generated, only **~457 MB** were shuffled across the network.

This corresponds to a **reduction of more than 90% in network traffic**, demonstrating that local aggregation is highly effective for Hebrew as well.

The slightly stronger reduction compared to English can be attributed to:
- Higher repetition of lexical items
- A smaller and more homogeneous vocabulary
- Stronger clustering of frequent expressions

---

## 3. Job 3 – Top-100 Aggregation Efficiency

Job 3 aggregates LLR-scored collocations and outputs only the Top-100 per decade and language.

### English – Job 3

| Metric | Value |
|------|------|
| Map input records | **182,274,626** |
| Map output records | 182,274,626 |
| Combine output records | **25,200** |
| Reduce input records | 25,200 |
| Reduce output records | **4,700** |
| Shuffle bytes | **~494 KB** |

**Observation:**  
The combiner dramatically reduces the shuffle size by pruning low-scoring collocations locally.  
This enables efficient global Top-K selection with minimal network cost.

---

### Hebrew – Job 3

| Metric | Value |
|------|------|
| Map input records | **26,449,631** |
| Map output records | 26,449,631 |
| Combine output records | **11,695** |
| Reduce input records | 11,695 |
| Reduce output records | **3,300** |
| Shuffle bytes | **~259 KB** |

**Observation:**  
Even for a smaller corpus, the combiner ensures that only high-quality candidates are transferred to reducers, maintaining scalability.

---

## 4. Communication Cost Summary

### Intermediate Record Reduction

| Job | Language | Reduction due to Combiner |
|----|---------|---------------------------|
| Job 1 | English | ~75% |
| Job 1 | Hebrew | ~78% |
| Job 3 | English | >99.9% |
| Job 3 | Hebrew | >99.9% |

---

## 5. Role and Effectiveness of the Combiner

### Combiner in Job 1 – Counting Phase

Job 1 performs large-scale counting of unigrams and bigrams and therefore generates massive amounts of intermediate data with many repeated keys.

The combiner in Job 1 performs **local aggregation of counts on each mapper**, summing occurrences of identical keys before the shuffle phase.

As a result:
- The number of intermediate records is reduced by approximately **75–80%**
- Network traffic is significantly reduced

Without the combiner, Job 1 would generate hundreds of millions of redundant key-value pairs.

---

### Combiner in Job 3 – Top-K Selection Phase

Job 3 implements a **Top-100 selection algorithm** per (language, decade) based on LLR scores.

Unlike Job 1, Job 3 does not aggregate counts. Instead, the combiner maintains a **local Top-K heap** and discards low-scoring collocations early.

The combiner in Job 3:
- Limits each mapper to emitting at most 100 candidates
- Ensures that only high-quality candidates are sent to reducers
- Reduces shuffle volume by more than **99.9%**

This design enables a **scalable Top-K algorithm**, where shuffle size is bounded and independent of corpus size.

---

## 6. Conclusion

The use of **local aggregation (combiners)** is critical for scalable collocation extraction from large corpora such as Google N-Grams.

- In **Job 1**, combiners reduce shuffle traffic by **3–4×**, preventing network bottlenecks.
- In **Job 3**, combiners enable an efficient Top-100 computation by reducing hundreds of millions of candidates to a few thousand.
- The results clearly demonstrate compliance with the assignment requirement to avoid redundant key-value generation and unnecessary memory assumptions.

Overall, the system scales efficiently for both English and Hebrew, and the statistical evidence confirms the effectiveness of the MapReduce design.

